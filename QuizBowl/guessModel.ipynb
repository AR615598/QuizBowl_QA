{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import contextGenerator\n",
    "import utils\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna.storages import RDBStorage\n",
    "import re\n",
    "from time import perf_counter_ns\n",
    "import itertools\n",
    "from datasets import load_from_disk, load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    TrainingArguments, Trainer, BatchEncoding,\n",
    "    DistilBertTokenizerFast, DefaultDataCollator, DistilBertForQuestionAnswering, \n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = 'distilbert-base-cased-distilled-squad'\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(checkpoint)\n",
    "contextGen = contextGenerator.LuceneRetrieval()\n",
    "\n",
    "\n",
    "try:\n",
    "    ds1 = load_from_disk('../res/data/QANTA-IgnoreIMP')\n",
    "    ds2 = load_from_disk('../res/data/QANTA-IncludeNA')\n",
    "    ds3 = load_from_disk('../res/data/guess_train')\n",
    "    \n",
    "except:\n",
    "    ds = load_dataset(\"community-datasets/qanta\", \"mode=first,char_skip=25\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data \n",
    "Given how BERT is a extractive model it will attempt to highlight its prediction in the provided context. In other words our task is to fine tune the model to predict the start and end positions of the answer in the context.  \n",
    "#### 1. Retreive context\n",
    "For each question we will need a relevent document where the answer may exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    ds['guesstest']['context']\n",
    "except:\n",
    "    ds = ds.map(lambda x: {'context':  contextGen(x['full_question'], 1)[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96221/96221 [00:17<00:00, 5483.00 examples/s]\n",
      "Map: 100%|██████████| 16706/16706 [00:03<00:00, 5076.99 examples/s]\n",
      "Map: 100%|██████████| 1055/1055 [00:00<00:00, 5606.27 examples/s]\n",
      "Map: 100%|██████████| 1161/1161 [00:00<00:00, 5665.19 examples/s]\n",
      "Map: 100%|██████████| 2151/2151 [00:00<00:00, 4463.02 examples/s]\n",
      "Map: 100%|██████████| 1953/1953 [00:00<00:00, 5404.71 examples/s]\n",
      "Map: 100%|██████████| 1145/1145 [00:00<00:00, 5758.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def reformat_context(row):\n",
    "    context = row['context']\n",
    "    contents = re.sub('\\n', ' ', context['contents'])\n",
    "    new = contextGen.remove_adj_dup(contents)\n",
    "    row['context']['contents'] = new\n",
    "    return row['context']\n",
    "\n",
    "ds = ds.map(lambda x: {'context':  reformat_context(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Find the start and end postions\n",
    "The contexts and questions are just strings to so we need to find the positions for the answers in the context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96221/96221 [00:18<00:00, 5141.64 examples/s]\n",
      "Map: 100%|██████████| 16706/16706 [00:03<00:00, 5461.68 examples/s]\n",
      "Map: 100%|██████████| 1055/1055 [00:00<00:00, 5314.49 examples/s]\n",
      "Map: 100%|██████████| 1161/1161 [00:00<00:00, 5898.43 examples/s]\n",
      "Map: 100%|██████████| 2151/2151 [00:00<00:00, 4473.73 examples/s]\n",
      "Map: 100%|██████████| 1953/1953 [00:00<00:00, 6093.68 examples/s]\n",
      "Map: 100%|██████████| 1145/1145 [00:00<00:00, 5938.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    ds['test']['char_pos']\n",
    "except:\n",
    "    ds = ds.map(lambda x: {'char_pos':  utils.term_char_index(x['answer'], x['context']['contents'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenize context/question pair and find the token positions\n",
    "Ensure the context comes first in the pair to align the character index with the token index. BERT limits the combined token count of context and question to 512. Since the context is capped at 400 words, this won’t cause issues, but we’ll use padding and truncation for consistency and edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96221/96221 [04:07<00:00, 388.22 examples/s] \n",
      "Map: 100%|██████████| 16706/16706 [00:40<00:00, 414.56 examples/s]\n",
      "Map: 100%|██████████| 1055/1055 [00:01<00:00, 559.13 examples/s]\n",
      "Map: 100%|██████████| 1161/1161 [00:02<00:00, 559.82 examples/s]\n",
      "Map: 100%|██████████| 2151/2151 [00:03<00:00, 555.78 examples/s]\n",
      "Map: 100%|██████████| 1953/1953 [00:03<00:00, 563.91 examples/s]\n",
      "Map: 100%|██████████| 1145/1145 [00:01<00:00, 590.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "unpack = lambda x, y, z: {\"start_positions\": x, \"end_positions\": y, \"encodings\": z}\n",
    "\n",
    "def tokenize_row(row: dict, tokenizer) -> dict[str, BatchEncoding]:\n",
    "    try: \n",
    "        encoding =  tokenizer(\n",
    "            text = row['context']['contents'], \n",
    "            text_pair = row['full_question'], \n",
    "            padding = 'max_length', \n",
    "            truncation = 'only_first', \n",
    "            max_length = 512, \n",
    "            return_tensors = 'pt', \n",
    "            padding_side = 'right',\n",
    "            return_length = False\n",
    "            )\n",
    "    except:\n",
    "        cleaned = utils.clean_text(row['full_question'])\n",
    "        encoding =  tokenizer(\n",
    "            text = row['context']['contents'], \n",
    "            text_pair = cleaned, \n",
    "            padding = 'max_length', \n",
    "            truncation = 'only_first', \n",
    "            max_length = 512, \n",
    "            return_tensors = 'pt', \n",
    "            padding_side = 'right',\n",
    "            return_length = False\n",
    "            )\n",
    "    start_pos = []\n",
    "    end_pos = []\n",
    "    for (x, y) in row['char_pos']:\n",
    "        st = encoding.char_to_token(x)\n",
    "        try:\n",
    "            ed = encoding.char_to_token(y-1)\n",
    "        except:\n",
    "            ed = encoding.char_to_token(y)\n",
    "\n",
    "        if st != None and ed != None:\n",
    "            start_pos.append(st)\n",
    "            end_pos.append(ed)\n",
    "    if len(start_pos) == 0:\n",
    "        # # no answer set to the [CLS] token\n",
    "        start_pos.append(0)\n",
    "        end_pos.append(0)        \n",
    "        # no answer set to invalid\n",
    "        # start_pos.append(-1)\n",
    "        # end_pos.append(-1)\n",
    "    encoding.update({'start_positions': start_pos, 'end_positions': end_pos})\n",
    "    return {\"encodings\": encoding}\n",
    "\n",
    "try: \n",
    "    ds['test']['encodings']\n",
    "except:\n",
    "    ds = ds.map(lambda x: tokenize_row(x, tokenizer))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equ_len_pad(encs): \n",
    "    # I want to find the largest list of start positions, from this pa all of rest to meet this size\n",
    "    longest_len = max([len(x['start_positions']) for x in encs])\n",
    "    for x in encs: \n",
    "        x_len = len(x['start_positions'])\n",
    "        x['start_positions'] = x['start_positions'] + ([-1 for x in range(longest_len - x_len)])\n",
    "        x['end_positions'] = x['end_positions'] + ([-1 for x in range(longest_len - x_len)])\n",
    "        x['input_ids'] =  x['input_ids'][0]\n",
    "        x['attention_mask'] = x['attention_mask'][0]\n",
    "    return encs\n",
    "\n",
    "train = equ_len_pad(ds2['guesstrain']['encodings'])\n",
    "val = equ_len_pad(ds2['guessdev']['encodings'])\n",
    "test = equ_len_pad(ds2['guesstest']['encodings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_2 = equ_len_pad(ds1['guesstrain']['encodings'])\n",
    "val_2 = equ_len_pad(ds1['guessdev']['encodings'])\n",
    "test_2 = equ_len_pad(ds1['guesstest']['encodings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (3/3 shards): 100%|██████████| 96221/96221 [00:03<00:00, 29042.09 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1055/1055 [00:00<00:00, 113858.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2151/2151 [00:00<00:00, 177398.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "guessTrain = DatasetDict({\n",
    "    'train': ds2['guesstrain'],\n",
    "    'val': ds2['guessdev'],\n",
    "    'test': ds2['guesstest'],\n",
    "})\n",
    "guessTrain.save_to_disk('../res/data/QANTA-IncludeNA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "from datasets import Dataset\n",
    "import time\n",
    "class BartTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # generate model's guess\n",
    "        outputs = model(input_ids = inputs['input_ids'] , attention_mask = inputs['attention_mask'])\n",
    "        # find the model's predictions \n",
    "        start_yhat = torch.argmax(outputs['start_logits'], dim= 1)\n",
    "        end_yhat = torch.argmax(outputs['end_logits'], dim= 1)\n",
    "        # check if this is a possible target\n",
    "        # if it is set it as the target, else choose a random valid target \n",
    "        start_target = []\n",
    "        end_target = []\n",
    "\n",
    "        valid_start_targets = []\n",
    "        valid_end_targets = []\n",
    "\n",
    "        for idx, x in enumerate(inputs['start_positions']): \n",
    "            cur_start_targ = []\n",
    "            cur_end_targ = []\n",
    "\n",
    "            for idx2, y in enumerate(x): \n",
    "                if y != -1:\n",
    "                    cur_start_targ.append(y)\n",
    "                    cur_end_targ.append(inputs['end_positions'][idx][idx2])\n",
    "\n",
    "                else: \n",
    "                    break \n",
    "                \n",
    "                \n",
    "            valid_start_targets.append(cur_start_targ if cur_start_targ else [-1])\n",
    "            valid_end_targets.append(cur_end_targ if cur_end_targ else [-1])          \n",
    "            \n",
    "        for x in range(len(inputs['input_ids'])):\n",
    "            if start_yhat[x] in valid_start_targets[x]:\n",
    "                start_target.append(start_yhat[x])                \n",
    "                end_target.append(end_yhat[x])\n",
    "            else: \n",
    "                ran_int = np.random.randint(len(valid_start_targets[x]))\n",
    "                s_rand = valid_start_targets[x][ran_int]\n",
    "                e_rand = valid_end_targets[x][ran_int]\n",
    "     \n",
    "                start_target.append(s_rand)\n",
    "                end_target.append(e_rand)\n",
    "\n",
    "        \n",
    "        device = outputs['start_logits'].device\n",
    "        start_target = torch.tensor(start_target, dtype=torch.long, device=device)\n",
    "        end_target = torch.tensor(end_target, dtype=torch.long, device=device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        start_loss = loss_fct(outputs['start_logits'], start_target)\n",
    "        end_loss = loss_fct(outputs['end_logits'], end_target)\n",
    "        \n",
    "        total_loss = (start_loss + end_loss) / 2\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "    \n",
    "    \n",
    "def model_init():\n",
    "    return (DistilBertForQuestionAnswering.from_pretrained(checkpoint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the results and logs for analysis\n",
    "def reformat(all_tests, names):\n",
    "    formated = []\n",
    "    final = []\n",
    "    # join a, a+1 rows excluding last\n",
    "    for idx, cur in enumerate(all_tests): \n",
    "        try: \n",
    "            cur['test']\n",
    "        except:\n",
    "            cur.insert(0, \"test\", names[idx])\n",
    "        test_output = test_output = cur.iloc[[-1]].dropna(axis='columns')\n",
    "        final.append(test_output)\n",
    "        test_train_log = pd.DataFrame(cur.iloc[lambda x: x.index % 2 == 0])\n",
    "        test_train_log = test_train_log.drop(test_train_log.index[-1]).dropna(axis='columns')\n",
    "        test_eval_log = pd.DataFrame(cur.iloc[lambda x: x.index % 2 != 0]).dropna(axis='columns')\n",
    "        formated.append(test_train_log.merge(test_eval_log, on = ['test', 'epoch', 'step']))\n",
    "    final = pd.concat(final)\n",
    "    return final, formated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "DISTILBERT_DROPOUT = 0.4\n",
    "DISTILBERT_ATT_DROPOUT = 0.4\n",
    "\n",
    "mini_train = train[:7500]\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(checkpoint)\n",
    "earlyStop = EarlyStoppingCallback(early_stopping_patience= 5, early_stopping_threshold=.1)\n",
    "    \n",
    "def model_init():\n",
    "    return (DistilBertForQuestionAnswering.from_pretrained(checkpoint, dropout=DISTILBERT_DROPOUT, attention_dropout=DISTILBERT_ATT_DROPOUT))\n",
    "\n",
    "# If regression or non-stratified case\n",
    "X_train_subset, _ = train_test_split(train,test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8, 16]),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2, 4, 8]),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [2, 4, 8, 16]),\n",
    "        \"eval_accumulation_steps\": trial.suggest_categorical(\"eval_accumulation_steps\", [1, 2, 4, 8]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.1, 0.6, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.01, 0.1, log=True), \n",
    "        \"lr_scheduler_type\": trial.suggest_categorical(\"lr_scheduler_type\", [\"linear\", \"cosine\", \"cosine_with_restarts\", \"inverse_sqrt\"]),\n",
    "}\n",
    "\n",
    "def model_init(trial):\n",
    "    if trial is not None:\n",
    "        DISTILBERT_DROPOUT = trial.suggest_float(\"dropout\", 0.1, 0.6, log=True)\n",
    "        DISTILBERT_ATT_DROPOUT = trial.suggest_float(\"dropout\", 0.1, 0.6, log=True)\n",
    "    else:\n",
    "        DISTILBERT_DROPOUT = 0\n",
    "        DISTILBERT_ATT_DROPOUT = 0\n",
    "        \n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    else:\n",
    "            mps_device = torch.device(\"mps\")\n",
    "\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(checkpoint, dropout=DISTILBERT_DROPOUT, attention_dropout=DISTILBERT_ATT_DROPOUT)\n",
    "    model.to(mps_device)\n",
    "\n",
    "    return model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    logging_steps=128,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=128,\n",
    "    save_steps=512,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = BartTrainer(\n",
    "    model_init=model_init,\n",
    "    model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=X_train_subset,\n",
    "    eval_dataset=val,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "storage = RDBStorage(url=\"sqlite:///QBAM_study.db\")\n",
    "    \n",
    "    \n",
    "# best_trial = trainer.hyperparameter_search(\n",
    "#     backend=\"optuna\",\n",
    "#     hp_space=optuna_hp_space,\n",
    "#     n_trials=0,\n",
    "#     load_if_exists=True,\n",
    "#     study_name=\"QBAM_study\",\n",
    "#     storage=storage,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "\n",
    "    DISTILBERT_DROPOUT = 0.4\n",
    "    DISTILBERT_ATT_DROPOUT = 0.4\n",
    "    \n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    else:\n",
    "            mps_device = torch.device(\"mps\")\n",
    "\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(checkpoint, dropout=DISTILBERT_DROPOUT, attention_dropout=DISTILBERT_ATT_DROPOUT)\n",
    "    model.to(mps_device)\n",
    "\n",
    "    return model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # General Training Settings\n",
    "    output_dir=\"../res/models/optuna_QBAM\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=128,\n",
    "    \n",
    "    # Batch Size & Accumulation\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_accumulation_steps=4,\n",
    "    \n",
    "    # Learning Rate and Scheduler\n",
    "    learning_rate= 0.0001295,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"inverse_sqrt\",\n",
    "    \n",
    "    # Weight Decay & Regularization\n",
    "    weight_decay=0.012,\n",
    "    \n",
    "    # Checkpoints & Saving\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"best\",\n",
    "    eval_steps=128,\n",
    "    save_steps=512,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Metrics & Evaluation\n",
    "    include_for_metrics=['loss'],\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    # Hub\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Trainer Initialization\n",
    "trainer = BartTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Training and Model Saving\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(\"../res/models/optuna_QBAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/envs/QBAM/lib/python3.11/site-packages/transformers/trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/Users/alex/anaconda3/envs/QBAM/lib/python3.11/site-packages/transformers/trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3006' max='3006' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3006/3006 2:23:04, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.749141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>0.676600</td>\n",
       "      <td>0.762972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>0.716213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>0.706112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2176</td>\n",
       "      <td>0.631900</td>\n",
       "      <td>0.651514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>0.611500</td>\n",
       "      <td>0.692766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2432</td>\n",
       "      <td>0.627300</td>\n",
       "      <td>0.649843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.646807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2688</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.649976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2816</td>\n",
       "      <td>0.615700</td>\n",
       "      <td>0.635251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2944</td>\n",
       "      <td>0.552100</td>\n",
       "      <td>0.682457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_init():\n",
    "\n",
    "    DISTILBERT_DROPOUT = 0.4\n",
    "    DISTILBERT_ATT_DROPOUT = 0.4\n",
    "    \n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    else:\n",
    "            mps_device = torch.device(\"mps\")\n",
    "\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(checkpoint, dropout=DISTILBERT_DROPOUT, attention_dropout=DISTILBERT_ATT_DROPOUT)\n",
    "    model.to(mps_device)\n",
    "\n",
    "    return model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # General Training Settings\n",
    "    output_dir=\"../res/models/optuna_IgnoreIMP\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=128,\n",
    "    \n",
    "    # Batch Size & Accumulation\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_accumulation_steps=4,\n",
    "    \n",
    "    # Learning Rate and Scheduler\n",
    "    learning_rate= 0.0001295,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"inverse_sqrt\",\n",
    "    \n",
    "    # Weight Decay & Regularization\n",
    "    weight_decay=0.012,\n",
    "    \n",
    "    # Checkpoints & Saving\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"best\",\n",
    "    eval_steps=128,\n",
    "    save_steps=512,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Metrics & Evaluation\n",
    "    include_for_metrics=['loss'],\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    # Hub\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Trainer Initialization\n",
    "trainer = BartTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_2,\n",
    "    eval_dataset=val_2,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Training and Model Saving\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(\"../res/models/optuna_IgnoreIMP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/anaconda3/envs/QBAM/lib/python3.11/site-packages/transformers/trainer.py:3423: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/Users/alex/anaconda3/envs/QBAM/lib/python3.11/site-packages/transformers/trainer.py:3119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3006' max='3006' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3006/3006 2:01:39, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>0.931100</td>\n",
       "      <td>0.961340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2176</td>\n",
       "      <td>0.898600</td>\n",
       "      <td>1.032816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2304</td>\n",
       "      <td>0.881800</td>\n",
       "      <td>0.924747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2432</td>\n",
       "      <td>0.890100</td>\n",
       "      <td>0.908818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.885000</td>\n",
       "      <td>0.952159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2688</td>\n",
       "      <td>0.839500</td>\n",
       "      <td>0.961150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2816</td>\n",
       "      <td>0.874500</td>\n",
       "      <td>0.976212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2944</td>\n",
       "      <td>0.784400</td>\n",
       "      <td>0.956210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_init():\n",
    "\n",
    "    DISTILBERT_DROPOUT = 0.4\n",
    "    DISTILBERT_ATT_DROPOUT = 0.4\n",
    "    \n",
    "    if not torch.backends.mps.is_available():\n",
    "        if not torch.backends.mps.is_built():\n",
    "            print(\"MPS not available because the current PyTorch install was not \"\n",
    "                \"built with MPS enabled.\")\n",
    "        else:\n",
    "            print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "    else:\n",
    "            mps_device = torch.device(\"mps\")\n",
    "\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(checkpoint, dropout=DISTILBERT_DROPOUT, attention_dropout=DISTILBERT_ATT_DROPOUT)\n",
    "    model.to(mps_device)\n",
    "\n",
    "    return model\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # General Training Settings\n",
    "    output_dir=\"../res/models/optuna_IncludeNA\",\n",
    "    num_train_epochs=2,\n",
    "    logging_steps=128,\n",
    "    \n",
    "    # Batch Size & Accumulation\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_accumulation_steps=4,\n",
    "    \n",
    "    # Learning Rate and Scheduler\n",
    "    learning_rate= 0.0001295,\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"inverse_sqrt\",\n",
    "    \n",
    "    # Weight Decay & Regularization\n",
    "    weight_decay=0.012,\n",
    "    \n",
    "    # Checkpoints & Saving\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"best\",\n",
    "    eval_steps=128,\n",
    "    save_steps=512,\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Metrics & Evaluation\n",
    "    include_for_metrics=['loss'],\n",
    "    metric_for_best_model=\"loss\",\n",
    "    \n",
    "    # Hub\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Trainer Initialization\n",
    "trainer = BartTrainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    data_collator=DefaultDataCollator(),\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Training and Model Saving\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(\"../res/models/optuna_IncludeNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_final_answer(text): \n",
    "    words = {}\n",
    "    final = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in words:\n",
    "            words[word] = word\n",
    "            final.append(word)\n",
    "        else: \n",
    "            if final[-1] == \"The\" or final[-1] == \"A\":\n",
    "                final.pop(-1)\n",
    "            break\n",
    "    return \"_\".join(final)\n",
    "\n",
    "\n",
    "\n",
    "def valid_spans(st_pos, ed_pos, k):\n",
    "        top_k_idx_start = np.argpartition(st_pos, range(-k, 0, 1), None)[-k:]\n",
    "        top_k_idx_end = np.argpartition(ed_pos, range(-k, 0, 1), None)[-k:]\n",
    "        zeroes = None\n",
    "        if 0 in top_k_idx_start or 0 in top_k_idx_end:\n",
    "            top_k_idx_start = np.delete(top_k_idx_start, np.where(top_k_idx_start == 0))\n",
    "            top_k_idx_end = np.delete(top_k_idx_end, np.where(top_k_idx_end == 0))\n",
    "            zeroes = [(0,0)]\n",
    "            \n",
    "        try:\n",
    "            pair_matrix = list(itertools.product(top_k_idx_start, top_k_idx_end)) + zeroes\n",
    "        except:\n",
    "             pair_matrix = list(itertools.product(top_k_idx_start, top_k_idx_end))\n",
    "             \n",
    "        for x in pair_matrix: \n",
    "            st, ed = x\n",
    "            if st > ed: \n",
    "                pair_matrix.remove(x)\n",
    "        score_matrix = np.full(len(pair_matrix), np.NINF)\n",
    "\n",
    "        for i, pair in enumerate(pair_matrix):\n",
    "            start, end = pair\n",
    "            score_matrix[i] = st_pos[0,start] + ed_pos[0,end]\n",
    "        \n",
    "        lst = ([pair_matrix[x] for x in np.argpartition(score_matrix, range(-k, 0, 1), None)[-k:]])\n",
    "        lst.reverse()\n",
    "        \n",
    "        return lst\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \n",
    "\\text{recall} = \\frac{\\text{\\# of matches}}{\\text{\\# of terms in ground truth}}\n",
    "$$\n",
    "$$\n",
    "\\text{precision} = \\frac{\\text{\\# of matches}}{\\text{\\# of terms in pred}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, k, tokenizer, data):\n",
    "    exact_match_with_ans = 0\n",
    "    exact_match_no_ans = 0\n",
    "    num_imp = 0\n",
    "    num_no_ans = 0\n",
    "    num_with_ans = 0\n",
    "    F1 = []\n",
    "    time = []\n",
    "    top_k_acc = []\n",
    "    precsion = []\n",
    "    recall = []\n",
    "\n",
    "    for x in data:\n",
    "        test_question = x\n",
    "        s_time = perf_counter_ns()\n",
    "        question, text = (\n",
    "            test_question[\"full_question\"],\n",
    "            test_question[\"context\"][\"contents\"],\n",
    "        )\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                text=text,\n",
    "                text_pair=question,\n",
    "                padding=\"max_length\",\n",
    "                truncation=\"only_first\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "                padding_side=\"right\",\n",
    "            )\n",
    "        except:\n",
    "            cleaned = utils.clean_text(question)\n",
    "            inputs = tokenizer(\n",
    "                text=text,\n",
    "                text_pair=cleaned,\n",
    "                padding=\"max_length\",\n",
    "                truncation=\"only_first\",\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "                padding_side=\"right\",\n",
    "                return_length=True,\n",
    "            )\n",
    "        outputs = model(**inputs)\n",
    "        e_time = perf_counter_ns()\n",
    "        time.append(e_time - s_time)\n",
    "\n",
    "        top_k = valid_spans(\n",
    "            outputs.start_logits.detach(), outputs.end_logits.detach(), k\n",
    "        )\n",
    "        answer_start_index, answer_end_index = top_k[0]\n",
    "        start_indexes = x[\"encodings\"][\"start_positions\"]\n",
    "        end_indexes = x[\"encodings\"][\"end_positions\"]\n",
    "\n",
    "\n",
    "        # exact match\n",
    "        if start_indexes[0] == 0:\n",
    "            if (answer_start_index in start_indexes) and (end_indexes[start_indexes.index(answer_start_index)] == answer_end_index):\n",
    "                exact_match_no_ans += 1\n",
    "        elif (answer_start_index in start_indexes) and (end_indexes[start_indexes.index(answer_start_index)] == answer_end_index):\n",
    "                exact_match_with_ans += 1\n",
    "            \n",
    "        if start_indexes[0] == -1:\n",
    "            num_imp += 1  \n",
    "        elif start_indexes[0] == 0:\n",
    "            num_no_ans += 1\n",
    "        else: \n",
    "            num_with_ans += 1\n",
    "\n",
    "\n",
    "        pred_span = set(range(answer_start_index, (answer_end_index + 1)))\n",
    "\n",
    "        matches = 0\n",
    "        for i, curr_ans in enumerate(start_indexes):\n",
    "            if curr_ans == -1:\n",
    "                continue\n",
    "\n",
    "            if answer_start_index > answer_end_index:\n",
    "                cur_matches = 0\n",
    "            else:\n",
    "                cur_span = set(range(curr_ans, (end_indexes[i] + 1)))\n",
    "                cur_matches = len(pred_span & cur_span)\n",
    "\n",
    "            if cur_matches > matches:\n",
    "                matches = cur_matches\n",
    "\n",
    "        prec = matches / (answer_end_index - answer_start_index + 1)\n",
    "        if np.isnan(prec):\n",
    "            print(matches, answer_end_index, answer_start_index)\n",
    "        rec = matches / (end_indexes[0] - start_indexes[0] + 1)\n",
    "\n",
    "        F1.append((2 * prec * rec) / (prec + rec)) if (prec + rec) != 0 else 0\n",
    "        precsion.append(prec)\n",
    "        recall.append(rec)\n",
    "\n",
    "        acc = 0\n",
    "        for start, end in top_k:\n",
    "            if (start in start_indexes) and (end in end_indexes):\n",
    "                acc = 1\n",
    "        \n",
    "        if start_indexes[0] != -1:\n",
    "            top_k_acc.append(acc)\n",
    "\n",
    "    if num_no_ans == 0:\n",
    "        no_ans_acc = 0\n",
    "    else:\n",
    "        no_ans_acc = exact_match_no_ans / num_no_ans\n",
    "\n",
    "    scores = {\n",
    "        \"exact_match_no_ans\": no_ans_acc,\n",
    "        \"exact_match_with_ans\": exact_match_with_ans / num_with_ans,\n",
    "        \"num_no_ans\": num_no_ans,\n",
    "        \"num_with_ans\": num_with_ans,\n",
    "        \"num_imp\": num_imp,\n",
    "        \"F1\": np.mean(F1),\n",
    "        \"time(ns)\": np.mean(time),\n",
    "        f\"top_{k}_acc\": np.mean(top_k_acc),\n",
    "        \"recall\": np.mean(recall),\n",
    "        \"precision\": np.mean(precsion),\n",
    "    }\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match_no_ans': 0,\n",
       " 'exact_match_with_ans': 0.6703601108033241,\n",
       " 'num_no_ans': 0,\n",
       " 'num_with_ans': 1444,\n",
       " 'num_imp': 707,\n",
       " 'F1': 0.90692353,\n",
       " 'time(ns)': 70900623.83170618,\n",
       " 'top_5_acc': 0.7901662049861495,\n",
       " 'recall': 0.5611749426254308,\n",
       " 'precision': 0.5089982}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_IgnoreIMP = DistilBertForQuestionAnswering.from_pretrained(\"../res/models/optuna_IgnoreIMP\")\n",
    "optuna_IgnoreIMP_tokenizer = DistilBertTokenizerFast.from_pretrained(\"../res/models/optuna_IgnoreIMP\")\n",
    "\n",
    "eval(optuna_IgnoreIMP, 5, optuna_IgnoreIMP_tokenizer, ds1['guesstest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was trained on a dataset that excludes any questions where the answer does not appear within the pre-generated context. As a result, the model only needs to learn to identify the most likely answer tokens. It achieved an accuracy of around 67%. However, this figure does not reflect real-world performance, as it assumes the context always contains the correct answer. In practice, this is not guaranteed. For instance, in the test set, 33% of the contexts did not include the correct answer, though the overall average context hit rate is about 83%. The model’s top-5 accuracy was around 80%, meaning that in most cases, at least one of the top five predicted spans was an exact match with the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match_no_ans': 0.9533239038189534,\n",
       " 'exact_match_with_ans': 0.036011080332409975,\n",
       " 'num_no_ans': 707,\n",
       " 'num_with_ans': 1444,\n",
       " 'num_imp': 0,\n",
       " 'F1': 0.9438061129263381,\n",
       " 'time(ns)': 62142272.69688517,\n",
       " 'top_5_acc': 0.7754532775453278,\n",
       " 'recall': 0.35977064931039826,\n",
       " 'precision': 0.3400126411593621}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_IncludeNA = DistilBertForQuestionAnswering.from_pretrained(\"../res/models/optuna_IncludeNA\")\n",
    "optuna_IncludeNA_tokenizer = DistilBertTokenizerFast.from_pretrained(\"../res/models/optuna_IncludeNA\")\n",
    "eval(optuna_IncludeNA, 5, optuna_IncludeNA_tokenizer, ds2['guesstest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was trained on a dataset that, instead of ignoring unanswerable questions, required the model to either find the most likely answer span or determine that the context did not contain the answer. This increased the task complexity, making the decision process more difficult. As a result, the model struggled to perform both tasks effectively.\n",
    "\n",
    "It achieved 95% accuracy in identifying contexts with no answer, which is a strong result. However, its accuracy for correctly answering questions with an answer present was only 3%. This suggests that the model is either defaulting to predicting “no answer” for most questions or is correctly identifying unanswerable cases but failing to extract correct answer spans when the answer is present.\n",
    "\n",
    "Both precision and recall were low, indicating poor overlap between the predicted spans and the true answer spans. This generally reflects poor span prediction performance. While the model’s top-5 accuracy was comparable to that of the first model, it’s important to note that this includes unanswerable questions, which make up about a third of the dataset. When considering only answerable questions, the effective top-5 accuracy drops to around 50%. Even so, identifying the unanswerable questions is an improtant task, and if the model can effectivly make this distinction It can be used as a intermediary model that identifies if the context needs to be regenerated, improving overall performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match_no_ans': 0,\n",
       " 'exact_match_with_ans': 0.27303523035230354,\n",
       " 'num_no_ans': 0,\n",
       " 'num_with_ans': 1476,\n",
       " 'num_imp': 675,\n",
       " 'F1': 0.69548494,\n",
       " 'time(ns)': 63985076.74012087,\n",
       " 'top_5_acc': 0.7947154471544715,\n",
       " 'recall': 0.5808254558603234,\n",
       " 'precision': 0.35171145}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna_QBAM = DistilBertForQuestionAnswering.from_pretrained(\"../res/models/optuna_QBAM\")\n",
    "optuna_QBAM_tokenizer = DistilBertTokenizerFast.from_pretrained(\"../res/models/optuna_QBAM\")\n",
    "eval(optuna_QBAM, 5, optuna_QBAM_tokenizer, ds3['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was trained using the same techniques and methodology as the first model, which ignored unanswerable questions. However, the dataset used here had a duplication issue: the title and the first term of the body text were often identical, and placed directly adjacent to each other. This caused confusion for the model, making it difficult to determine the correct start and end positions for answer spans. As a result, it produced redundant predictions such as “Texas, The state Texas”.\n",
    "\n",
    "This duplication affected precision more than recall. Since the ground truth and predicted spans often included similar tokens, the recall remained relatively stable. However, precision suffered because the predictions contained excessive tokens resulting in the precsion essentially halved, given how precision is a function of matches over prediciton span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match_no_ans': 0,\n",
       " 'exact_match_with_ans': 0.0,\n",
       " 'num_no_ans': 0,\n",
       " 'num_with_ans': 1476,\n",
       " 'num_imp': 675,\n",
       " 'F1': 0.29166666,\n",
       " 'time(ns)': 62484182.24918643,\n",
       " 'top_5_acc': 0.09688346883468835,\n",
       " 'recall': 0.0005578800557880056,\n",
       " 'precision': 0.0005313143418146698}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_QBAM = DistilBertForQuestionAnswering.from_pretrained(checkpoint)\n",
    "squad_QBAM_tokenizer = DistilBertTokenizerFast.from_pretrained(checkpoint)\n",
    "eval(squad_QBAM, 5, squad_QBAM_tokenizer, ds3['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when evaluating a general-purpose pretrained model not designed for pyramid-style questions, the performance was very poor—it almost never predicted any of the correct answer spans. This highlights the importance of training models specifically for the task at hand. The improvement seen with the specialized model suggests that further specialization could yield even better results. However, given the nature of the Quiz Bowl format, where questions gradually reveal information and vary significantly in structure, developing highly specialized models may not be practical or scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def rand_example(k:int, data, model, tokenizer):\n",
    "    examples = [data[int(x)] for x in (np.random.default_rng().integers(low=0, high=(len(data) - 1), size=k))]\n",
    "    for quest in examples:\n",
    "        question, text = quest['full_question'], quest['context']['contents']\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                text = text, \n",
    "                text_pair=question, \n",
    "                padding = 'max_length', \n",
    "                truncation = 'only_first', \n",
    "                max_length = 512, \n",
    "                return_tensors = 'pt', \n",
    "                padding_side = 'right'\n",
    "                )\n",
    "        except:\n",
    "            cleaned = utils.clean_text(question)\n",
    "            inputs =  tokenizer(\n",
    "                text = text,\n",
    "                text_pair = cleaned, \n",
    "                padding = 'max_length', \n",
    "                truncation = 'only_first', \n",
    "                max_length = 512, \n",
    "                return_tensors = 'pt', \n",
    "                padding_side = 'right',\n",
    "                return_length = True\n",
    "                )\n",
    "        outputs = model(**inputs)\n",
    "        answer_start_index = None\n",
    "        answer_end_index = None\n",
    "        \n",
    "        top_k = valid_spans(outputs.start_logits.detach(), outputs.end_logits.detach(), k)\n",
    "        answer_start_index, answer_end_index= top_k[0]\n",
    "        \n",
    "        if answer_start_index == None : \n",
    "            answer_start_index = 0\n",
    "            answer_end_index = 0                    \n",
    "        decoded_str = tokenizer.decode(inputs['input_ids'][0,answer_start_index:answer_end_index+ 1])\n",
    "\n",
    "\n",
    "        # pred = extract_final_answer(decoded_str)\n",
    "  \n",
    "        print(\"Question: \" + quest[\"first_sentence\"])\n",
    "        print(\"Answer: \" + quest['answer'])\n",
    "        print(\"Final Prediction: \"+ decoded_str)\n",
    "        print(f\"Score: {outputs.start_logits[0,answer_start_index]+ outputs.end_logits[0,answer_end_index]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When this compound is added to proteins with aromatic rings, they turn yellow; that test makes use of the xanthoproteic reaction.\n",
      "Answer: Nitric_acid\n",
      "Final Prediction: Nitric acid\n",
      "Score: 20.90878677368164\n",
      "\n",
      "Question: This novel gave rise to a namesake genre of books, one of which by Philip Cozans is named after a figure from this novel who has an aunt named Ophelia from Vermont and who teaches a girl named Topsy about God.\n",
      "Answer: Uncle_Tom's_Cabin\n",
      "Final Prediction: Uncle Tom ' s Cabin Cabin\n",
      "Score: 23.21484375\n",
      "\n",
      "Question: Pulsed amperometry is combined with this technique in analysis of sugars.\n",
      "Answer: High-performance_liquid_chromatography\n",
      "Final Prediction: Liquid chromatography – mass spectrometry\n",
      "Score: 12.616650581359863\n",
      "\n",
      "Question: In Chilean mythology, throwing the Trauco's staff in a fire will cause it to produce this substance which can be used to heal people who were hurt by the Trauco's curses.  \n",
      "Answer: Oil\n",
      "Final Prediction: silken mailcoat ” BULLET : : : : - Babr - e Bayan, a suit of armour that Rostam wore in wars described in the Persian epic \" Shahnameh \" The armour was invulnerable against fire, water\n",
      "Score: 7.708961009979248\n",
      "\n",
      "Question: In his Reminiscences, the Irish tenor Michael Kelly described fighting the composer of this opera for the right to stutter when performing his role.\n",
      "Answer: The_Marriage_of_Figaro\n",
      "Final Prediction: The Marriage of Figaro\n",
      "Score: 21.739212036132812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optuna_IgnoreIMP = DistilBertForQuestionAnswering.from_pretrained(\"../res/models/optuna_IgnoreIMP\")\n",
    "optuna_IgnoreIMP_tokenizer = DistilBertTokenizerFast.from_pretrained(\"../res/models/optuna_IgnoreIMP\")\n",
    "\n",
    "rand_example(5, ds1['guesstrain'], optuna_IgnoreIMP, optuna_IgnoreIMP_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QBAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
